{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Exploratory Data Analysis (EDA)\n",
        "\n",
        "## Financial News Sentiment Analysis - EDA\n",
        "\n",
        "This notebook performs comprehensive EDA on the financial news dataset including:\n",
        "- Descriptive Statistics\n",
        "- Text Analysis and Topic Modeling\n",
        "- Time Series Analysis\n",
        "- Publisher Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('../data/raw_analyst_ratings.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information about the dataset\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\\nDuplicate Rows:\")\n",
        "print(f\"Total duplicates: {df.duplicated().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop the Unnamed: 0 column if it exists\n",
        "if 'Unnamed: 0' in df.columns:\n",
        "    df = df.drop('Unnamed: 0', axis=1)\n",
        "\n",
        "# Convert date column to datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Extract date components for analysis\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "df['day_of_week'] = df['date'].dt.day_name()\n",
        "df['hour'] = df['date'].dt.hour\n",
        "df['date_only'] = df['date'].dt.date\n",
        "\n",
        "print(\"Data preprocessing completed!\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Descriptive Statistics\n",
        "\n",
        "### 3.1 Headline Length Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate headline length statistics\n",
        "df['headline_length'] = df['headline'].str.len()\n",
        "df['headline_word_count'] = df['headline'].str.split().str.len()\n",
        "\n",
        "print(\"Headline Length Statistics:\")\n",
        "print(\"=\"*50)\n",
        "print(df['headline_length'].describe())\n",
        "print(\"\\nHeadline Word Count Statistics:\")\n",
        "print(\"=\"*50)\n",
        "print(df['headline_word_count'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize headline length distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Character length distribution\n",
        "axes[0].hist(df['headline_length'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Distribution of Headline Character Length', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Character Count')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].axvline(df['headline_length'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df[\"headline_length\"].mean():.1f}')\n",
        "axes[0].legend()\n",
        "\n",
        "# Word count distribution\n",
        "axes[1].hist(df['headline_word_count'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
        "axes[1].set_title('Distribution of Headline Word Count', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Word Count')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].axvline(df['headline_word_count'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df[\"headline_word_count\"].mean():.1f}')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Articles per Publisher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count articles per publisher\n",
        "publisher_counts = df['publisher'].value_counts()\n",
        "\n",
        "print(\"Articles per Publisher (Top 20):\")\n",
        "print(\"=\"*50)\n",
        "print(publisher_counts.head(20))\n",
        "print(f\"\\nTotal unique publishers: {df['publisher'].nunique()}\")\n",
        "print(f\"Total articles: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top publishers\n",
        "plt.figure(figsize=(14, 8))\n",
        "top_publishers = publisher_counts.head(15)\n",
        "plt.barh(range(len(top_publishers)), top_publishers.values, color='steelblue')\n",
        "plt.yticks(range(len(top_publishers)), top_publishers.index)\n",
        "plt.xlabel('Number of Articles', fontsize=12)\n",
        "plt.title('Top 15 Publishers by Article Count', fontsize=16, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Publication Date Trends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Articles by year\n",
        "articles_by_year = df['year'].value_counts().sort_index()\n",
        "print(\"Articles by Year:\")\n",
        "print(\"=\"*50)\n",
        "print(articles_by_year)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Articles by month\n",
        "articles_by_month = df['month'].value_counts().sort_index()\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "print(\"Articles by Month:\")\n",
        "print(\"=\"*50)\n",
        "for month, count in articles_by_month.items():\n",
        "    print(f\"{month_names[month-1]}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Articles by day of week\n",
        "articles_by_dow = df['day_of_week'].value_counts()\n",
        "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "articles_by_dow = articles_by_dow.reindex([d for d in dow_order if d in articles_by_dow.index])\n",
        "\n",
        "print(\"Articles by Day of Week:\")\n",
        "print(\"=\"*50)\n",
        "print(articles_by_dow)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize temporal trends\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Articles by year\n",
        "axes[0, 0].plot(articles_by_year.index, articles_by_year.values, marker='o', linewidth=2, markersize=8)\n",
        "axes[0, 0].set_title('Articles Published by Year', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Year')\n",
        "axes[0, 0].set_ylabel('Number of Articles')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Articles by month\n",
        "axes[0, 1].bar(articles_by_month.index, articles_by_month.values, color='coral')\n",
        "axes[0, 1].set_title('Articles Published by Month', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Month')\n",
        "axes[0, 1].set_ylabel('Number of Articles')\n",
        "axes[0, 1].set_xticks(range(1, 13))\n",
        "axes[0, 1].set_xticklabels(month_names, rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Articles by day of week\n",
        "axes[1, 0].bar(range(len(articles_by_dow)), articles_by_dow.values, color='lightgreen')\n",
        "axes[1, 0].set_title('Articles Published by Day of Week', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Day of Week')\n",
        "axes[1, 0].set_ylabel('Number of Articles')\n",
        "axes[1, 0].set_xticks(range(len(articles_by_dow)))\n",
        "axes[1, 0].set_xticklabels(articles_by_dow.index, rotation=45)\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Articles by hour\n",
        "articles_by_hour = df['hour'].value_counts().sort_index()\n",
        "axes[1, 1].bar(articles_by_hour.index, articles_by_hour.values, color='gold')\n",
        "axes[1, 1].set_title('Articles Published by Hour of Day', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Hour (UTC-4)')\n",
        "axes[1, 1].set_ylabel('Number of Articles')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Text Analysis and Topic Modeling\n",
        "\n",
        "### 4.1 Common Keywords and Phrases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare text for analysis\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Add financial domain-specific stop words\n",
        "financial_stopwords = {'stock', 'stocks', 'company', 'companies', 'market', 'markets', \n",
        "                      'price', 'prices', 'trading', 'trade', 'trades', 'day', 'week', \n",
        "                      'year', 'time', 'news', 'article', 'report', 'reports'}\n",
        "stop_words.update(financial_stopwords)\n",
        "\n",
        "def extract_keywords(text):\n",
        "    \"\"\"Extract keywords from text\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Filter out stopwords, punctuation, and short words\n",
        "    keywords = [token for token in tokens \n",
        "                if token.isalpha() and len(token) > 2 and token not in stop_words]\n",
        "    return keywords\n",
        "\n",
        "# Extract keywords from all headlines\n",
        "print(\"Extracting keywords from headlines...\")\n",
        "all_keywords = []\n",
        "for headline in df['headline']:\n",
        "    all_keywords.extend(extract_keywords(headline))\n",
        "\n",
        "# Count keyword frequency\n",
        "keyword_counts = Counter(all_keywords)\n",
        "top_keywords = keyword_counts.most_common(30)\n",
        "\n",
        "print(\"\\nTop 30 Keywords:\")\n",
        "print(\"=\"*50)\n",
        "for keyword, count in top_keywords:\n",
        "    print(f\"{keyword:20s}: {count:6d}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top keywords\n",
        "keywords_df = pd.DataFrame(top_keywords, columns=['Keyword', 'Count'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(range(len(keywords_df)), keywords_df['Count'].values, color='teal')\n",
        "plt.yticks(range(len(keywords_df)), keywords_df['Keyword'].values)\n",
        "plt.xlabel('Frequency', fontsize=12)\n",
        "plt.title('Top 30 Keywords in Financial News Headlines', fontsize=16, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create word cloud\n",
        "text_for_wordcloud = ' '.join(all_keywords)\n",
        "\n",
        "wordcloud = WordCloud(width=1200, height=600, background_color='white', \n",
        "                      max_words=100, colormap='viridis').generate(text_for_wordcloud)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Financial News Headlines', fontsize=18, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Extract Significant Events and Phrases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look for significant financial events/phrases\n",
        "significant_phrases = [\n",
        "    'fda approval', 'price target', 'earnings', 'revenue', 'profit', \n",
        "    'merger', 'acquisition', 'ipo', 'dividend', 'split', 'upgrade', \n",
        "    'downgrade', 'analyst', 'rating', 'forecast', 'guidance', \n",
        "    '52-week high', '52-week low', 'bullish', 'bearish'\n",
        "]\n",
        "\n",
        "phrase_counts = {}\n",
        "for phrase in significant_phrases:\n",
        "    count = df['headline'].str.lower().str.contains(phrase, na=False).sum()\n",
        "    phrase_counts[phrase] = count\n",
        "\n",
        "phrase_df = pd.DataFrame(list(phrase_counts.items()), columns=['Phrase', 'Count'])\n",
        "phrase_df = phrase_df.sort_values('Count', ascending=False)\n",
        "\n",
        "print(\"Significant Financial Phrases in Headlines:\")\n",
        "print(\"=\"*50)\n",
        "print(phrase_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize significant phrases\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(range(len(phrase_df)), phrase_df['Count'].values, color='purple')\n",
        "plt.yticks(range(len(phrase_df)), phrase_df['Phrase'].values)\n",
        "plt.xlabel('Frequency', fontsize=12)\n",
        "plt.title('Significant Financial Phrases in Headlines', fontsize=16, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Time Series Analysis\n",
        "\n",
        "### 5.1 Publication Frequency Over Time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create time series of daily publication frequency\n",
        "daily_counts = df.groupby('date_only').size().reset_index(name='article_count')\n",
        "daily_counts['date_only'] = pd.to_datetime(daily_counts['date_only'])\n",
        "daily_counts = daily_counts.sort_values('date_only')\n",
        "\n",
        "print(f\"Date range: {daily_counts['date_only'].min()} to {daily_counts['date_only'].max()}\")\n",
        "print(f\"Total days with articles: {len(daily_counts)}\")\n",
        "print(f\"\\nDaily publication statistics:\")\n",
        "print(daily_counts['article_count'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot daily publication frequency\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.plot(daily_counts['date_only'], daily_counts['article_count'], linewidth=1, alpha=0.7)\n",
        "plt.title('Daily Publication Frequency Over Time', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monthly aggregation for better visualization\n",
        "df['year_month'] = df['date'].dt.to_period('M')\n",
        "monthly_counts = df.groupby('year_month').size().reset_index(name='article_count')\n",
        "monthly_counts['year_month'] = monthly_counts['year_month'].astype(str)\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.bar(range(len(monthly_counts)), monthly_counts['article_count'].values, \n",
        "        color='steelblue', alpha=0.7)\n",
        "plt.title('Monthly Publication Frequency', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(range(0, len(monthly_counts), max(1, len(monthly_counts)//20)), \n",
        "           monthly_counts['year_month'][::max(1, len(monthly_counts)//20)], \n",
        "           rotation=45)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Publishing Time Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publishing times\n",
        "print(\"Publishing Time Analysis:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Most common publishing hour: {df['hour'].mode()[0]}:00\")\n",
        "print(f\"\\nArticles by hour (top 10):\")\n",
        "print(df['hour'].value_counts().head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize publishing times\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Hourly distribution\n",
        "hour_counts = df['hour'].value_counts().sort_index()\n",
        "axes[0].bar(hour_counts.index, hour_counts.values, color='coral', alpha=0.7)\n",
        "axes[0].set_title('Article Publication by Hour of Day', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Hour (UTC-4)')\n",
        "axes[0].set_ylabel('Number of Articles')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Day of week distribution\n",
        "dow_counts = df['day_of_week'].value_counts()\n",
        "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "dow_counts = dow_counts.reindex([d for d in dow_order if d in dow_counts.index])\n",
        "axes[1].bar(range(len(dow_counts)), dow_counts.values, color='lightgreen', alpha=0.7)\n",
        "axes[1].set_title('Article Publication by Day of Week', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Day of Week')\n",
        "axes[1].set_ylabel('Number of Articles')\n",
        "axes[1].set_xticks(range(len(dow_counts)))\n",
        "axes[1].set_xticklabels(dow_counts.index, rotation=45)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Publisher Analysis\n",
        "\n",
        "### 6.1 Publisher Contribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed publisher statistics\n",
        "publisher_stats = df.groupby('publisher').agg({\n",
        "    'headline': 'count',\n",
        "    'stock': 'nunique',\n",
        "    'date': ['min', 'max']\n",
        "}).reset_index()\n",
        "\n",
        "publisher_stats.columns = ['publisher', 'article_count', 'unique_stocks', 'first_article', 'last_article']\n",
        "publisher_stats = publisher_stats.sort_values('article_count', ascending=False)\n",
        "\n",
        "print(\"Publisher Statistics (Top 20):\")\n",
        "print(\"=\"*70)\n",
        "print(publisher_stats.head(20).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Publisher contribution percentage\n",
        "total_articles = len(df)\n",
        "publisher_stats['contribution_pct'] = (publisher_stats['article_count'] / total_articles * 100).round(2)\n",
        "\n",
        "print(\"\\nPublisher Contribution (Top 15):\")\n",
        "print(\"=\"*70)\n",
        "top_15 = publisher_stats.head(15)[['publisher', 'article_count', 'contribution_pct']]\n",
        "print(top_15.to_string(index=False))\n",
        "\n",
        "print(f\"\\nTop 15 publishers account for {top_15['contribution_pct'].sum():.2f}% of all articles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Publisher Domain Analysis (if email addresses are used)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if publishers contain email addresses\n",
        "import re\n",
        "\n",
        "def extract_domain(text):\n",
        "    \"\"\"Extract domain from email or text\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return None\n",
        "    # Check for email pattern\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n",
        "    match = re.search(email_pattern, str(text))\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "# Extract domains from publishers\n",
        "df['publisher_domain'] = df['publisher'].apply(extract_domain)\n",
        "\n",
        "if df['publisher_domain'].notna().sum() > 0:\n",
        "    domain_counts = df['publisher_domain'].value_counts()\n",
        "    print(f\"Found {len(domain_counts)} unique domains from email addresses\")\n",
        "    print(\"\\nTop domains:\")\n",
        "    print(domain_counts.head(10))\n",
        "else:\n",
        "    print(\"No email addresses found in publisher field.\")\n",
        "    print(\"\\nAnalyzing publisher names as organizations instead...\")\n",
        "    \n",
        "    # Analyze unique publisher names\n",
        "    print(f\"\\nTotal unique publishers: {df['publisher'].nunique()}\")\n",
        "    print(f\"\\nPublisher name patterns (sample):\")\n",
        "    print(df['publisher'].value_counts().head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize publisher distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Top 20 publishers\n",
        "top_20_publishers = publisher_counts.head(20)\n",
        "axes[0].barh(range(len(top_20_publishers)), top_20_publishers.values, color='steelblue')\n",
        "axes[0].set_yticks(range(len(top_20_publishers)))\n",
        "axes[0].set_yticklabels(top_20_publishers.index)\n",
        "axes[0].set_xlabel('Number of Articles')\n",
        "axes[0].set_title('Top 20 Publishers by Article Count', fontsize=14, fontweight='bold')\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Publisher contribution pie chart (top 10)\n",
        "top_10_publishers = publisher_counts.head(10)\n",
        "others_count = publisher_counts.iloc[10:].sum()\n",
        "pie_data = list(top_10_publishers.values) + [others_count]\n",
        "pie_labels = list(top_10_publishers.index) + ['Others']\n",
        "\n",
        "axes[1].pie(pie_data, labels=pie_labels, autopct='%1.1f%%', startangle=90)\n",
        "axes[1].set_title('Publisher Distribution (Top 10 + Others)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Stock Coverage by Publisher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze which publishers cover which stocks\n",
        "publisher_stock = df.groupby(['publisher', 'stock']).size().reset_index(name='count')\n",
        "publisher_stock_pivot = publisher_stock.pivot(index='publisher', columns='stock', values='count').fillna(0)\n",
        "\n",
        "# Top publishers and their stock coverage\n",
        "top_publishers_list = publisher_counts.head(10).index.tolist()\n",
        "top_publishers_coverage = publisher_stock_pivot.loc[top_publishers_list]\n",
        "\n",
        "print(\"Stock Coverage by Top Publishers:\")\n",
        "print(\"=\"*70)\n",
        "print(top_publishers_coverage.to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize stock coverage heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(top_publishers_coverage, annot=True, fmt='.0f', cmap='YlOrRd', \n",
        "            cbar_kws={'label': 'Number of Articles'})\n",
        "plt.title('Stock Coverage Heatmap by Top Publishers', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Stock Symbol', fontsize=12)\n",
        "plt.ylabel('Publisher', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Key Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"EDA SUMMARY - KEY INSIGHTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n1. DATASET OVERVIEW:\")\n",
        "print(f\"   - Total articles: {len(df):,}\")\n",
        "print(f\"   - Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"   - Unique publishers: {df['publisher'].nunique()}\")\n",
        "print(f\"   - Unique stocks: {df['stock'].nunique()}\")\n",
        "\n",
        "print(f\"\\n2. HEADLINE STATISTICS:\")\n",
        "print(f\"   - Average headline length: {df['headline_length'].mean():.1f} characters\")\n",
        "print(f\"   - Average word count: {df['headline_word_count'].mean():.1f} words\")\n",
        "\n",
        "print(f\"\\n3. PUBLISHER INSIGHTS:\")\n",
        "print(f\"   - Top publisher: {publisher_counts.index[0]} ({publisher_counts.iloc[0]:,} articles)\")\n",
        "print(f\"   - Top 10 publishers account for {publisher_stats.head(10)['contribution_pct'].sum():.1f}% of articles\")\n",
        "\n",
        "print(f\"\\n4. TEMPORAL PATTERNS:\")\n",
        "print(f\"   - Most active year: {articles_by_year.idxmax()} ({articles_by_year.max():,} articles)\")\n",
        "print(f\"   - Most active month: {month_names[articles_by_month.idxmax()-1]} ({articles_by_month.max():,} articles)\")\n",
        "print(f\"   - Most active day: {articles_by_dow.index[0]} ({articles_by_dow.iloc[0]:,} articles)\")\n",
        "print(f\"   - Peak publishing hour: {df['hour'].mode()[0]}:00\")\n",
        "\n",
        "print(f\"\\n5. TEXT ANALYSIS:\")\n",
        "print(f\"   - Top keyword: '{top_keywords[0][0]}' (appears {top_keywords[0][1]:,} times)\")\n",
        "print(f\"   - Most common phrase: '{phrase_df.iloc[0]['Phrase']}' ({phrase_df.iloc[0]['Count']:,} occurrences)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
